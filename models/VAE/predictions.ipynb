{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE import VAE\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils import load_csv, dataloader, load_save_models\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from preprocessing.tiff_handling import handle_tiff\n",
    "import plotly.express as px\n",
    "from stats import evaluate_perf_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 436\n",
    "input_dim = 4800\n",
    "hidden_dim = 4000\n",
    "latent_dim = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_215065/2905130577.py:2: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(input_dim,hidden_dim,latent_dim)\n",
    "VAE.load_state_dict(model,state_dict=torch.load('./vae.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in data necessary for computing the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bendm/machine_learning/project/majority-vote-outlier-detection/models/VAE/../../utils/load_csv.py:20: DtypeWarning:\n",
      "\n",
      "Columns (91,94,209,213) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier test validation: False\n"
     ]
    }
   ],
   "source": [
    "set = load_csv.load_pandas()\n",
    "train, val, test = load_csv.split_data(set)\n",
    "\n",
    "print(f'outlier test validation: {any(val[\"classification\"] != 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bendm/machine_learning/project/majority-vote-outlier-detection/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning:\n",
      "\n",
      "The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=4800, out_features=4000, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=4000, out_features=2000, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (7): SiLU()\n",
       "    (8): Linear(in_features=500, out_features=640, bias=True)\n",
       "  )\n",
       "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=500, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=500, out_features=1000, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=1000, out_features=2000, bias=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=2000, out_features=4000, bias=True)\n",
       "    (7): SiLU()\n",
       "    (8): Linear(in_features=4000, out_features=4800, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Resize((60, 80)),\n",
    "        v2.ToTensor(),\n",
    "        v2.Lambda(\n",
    "            lambda x: (x.view(-1) - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_set = dataloader.ImagePathDataset(val, transform)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>file_names</th>\n",
       "      <th>label</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0816), tensor(0.0795), tensor(0.0970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1591), tensor(0.1353), tensor(0.1500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1166), tensor(0.1164), tensor(0.1190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1287), tensor(0.1243), tensor(0.1208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1595), tensor(0.1544), tensor(0.1538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0396), tensor(0.0453), tensor(0.0609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9812</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Doublet</td>\n",
       "      <td>[tensor(0.1147), tensor(0.1244), tensor(0.1273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Doublet</td>\n",
       "      <td>[tensor(0.1007), tensor(0.1204), tensor(0.1291...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7724</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1171), tensor(0.0756), tensor(0.1007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0233), tensor(0.0371), tensor(0.0493...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2914 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      classification                                         file_names  \\\n",
       "51                 1  /home/bendm/machine_learning/project/majority-...   \n",
       "6622               1  /home/bendm/machine_learning/project/majority-...   \n",
       "9400               1  /home/bendm/machine_learning/project/majority-...   \n",
       "3022               1  /home/bendm/machine_learning/project/majority-...   \n",
       "1224               1  /home/bendm/machine_learning/project/majority-...   \n",
       "...              ...                                                ...   \n",
       "153               -1  /home/bendm/machine_learning/project/majority-...   \n",
       "9812               2  /home/bendm/machine_learning/project/majority-...   \n",
       "7403               2  /home/bendm/machine_learning/project/majority-...   \n",
       "7724              -1  /home/bendm/machine_learning/project/majority-...   \n",
       "8570              -1  /home/bendm/machine_learning/project/majority-...   \n",
       "\n",
       "        label                                              image  \n",
       "51    Singlet  [tensor(0.0816), tensor(0.0795), tensor(0.0970...  \n",
       "6622  Singlet  [tensor(0.1591), tensor(0.1353), tensor(0.1500...  \n",
       "9400  Singlet  [tensor(0.1166), tensor(0.1164), tensor(0.1190...  \n",
       "3022  Singlet  [tensor(0.1287), tensor(0.1243), tensor(0.1208...  \n",
       "1224  Singlet  [tensor(0.1595), tensor(0.1544), tensor(0.1538...  \n",
       "...       ...                                                ...  \n",
       "153   Singlet  [tensor(0.0396), tensor(0.0453), tensor(0.0609...  \n",
       "9812  Doublet  [tensor(0.1147), tensor(0.1244), tensor(0.1273...  \n",
       "7403  Doublet  [tensor(0.1007), tensor(0.1204), tensor(0.1291...  \n",
       "7724  Singlet  [tensor(0.1171), tensor(0.0756), tensor(0.1007...  \n",
       "8570  Singlet  [tensor(0.0233), tensor(0.0371), tensor(0.0493...  \n",
       "\n",
       "[2914 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"image\"] = test[\"file_names\"].apply(lambda x: transform(handle_tiff(x)))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-11 00:20:19,120] A new study created in memory with name: no-name-934c0d9c-6f8b-4944-833f-6058b623ac80\n"
     ]
    }
   ],
   "source": [
    "# optimize percentile of error threshold\n",
    "import optuna\n",
    "\n",
    "truth = [1 if value == 1 else 0 for value in test[\"classification\"]]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    perc = trial.suggest_int(\"percentile\", 0, 100)\n",
    "    model.extract_error_threshold(\n",
    "        val_dataloader,\n",
    "        method=\"percentile\",\n",
    "        value=perc,\n",
    "    )\n",
    "    output = model.predict(test)\n",
    "\n",
    "    perf_metrics = evaluate_perf_utils.evaluate_performance(truth, output[\"prediction\"])\n",
    "    return perf_metrics[\"f1\"]\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "#study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"best percentile: {study.best_params['percentile']}\")\n",
    "#print(f\"best f1: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing threshold: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>file_names</th>\n",
       "      <th>label</th>\n",
       "      <th>image</th>\n",
       "      <th>prediction</th>\n",
       "      <th>reconstruction_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0816), tensor(0.0795), tensor(0.0970...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.190615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1591), tensor(0.1353), tensor(0.1500...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.272947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1166), tensor(0.1164), tensor(0.1190...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1287), tensor(0.1243), tensor(0.1208...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.330162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1595), tensor(0.1544), tensor(0.1538...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.248079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0396), tensor(0.0453), tensor(0.0609...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9812</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Doublet</td>\n",
       "      <td>[tensor(0.1147), tensor(0.1244), tensor(0.1273...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.316925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Doublet</td>\n",
       "      <td>[tensor(0.1007), tensor(0.1204), tensor(0.1291...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7724</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.1171), tensor(0.0756), tensor(0.1007...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.365307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>-1</td>\n",
       "      <td>/home/bendm/machine_learning/project/majority-...</td>\n",
       "      <td>Singlet</td>\n",
       "      <td>[tensor(0.0233), tensor(0.0371), tensor(0.0493...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.320792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2914 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      classification                                         file_names  \\\n",
       "51                 1  /home/bendm/machine_learning/project/majority-...   \n",
       "6622               1  /home/bendm/machine_learning/project/majority-...   \n",
       "9400               1  /home/bendm/machine_learning/project/majority-...   \n",
       "3022               1  /home/bendm/machine_learning/project/majority-...   \n",
       "1224               1  /home/bendm/machine_learning/project/majority-...   \n",
       "...              ...                                                ...   \n",
       "153               -1  /home/bendm/machine_learning/project/majority-...   \n",
       "9812               2  /home/bendm/machine_learning/project/majority-...   \n",
       "7403               2  /home/bendm/machine_learning/project/majority-...   \n",
       "7724              -1  /home/bendm/machine_learning/project/majority-...   \n",
       "8570              -1  /home/bendm/machine_learning/project/majority-...   \n",
       "\n",
       "        label                                              image  prediction  \\\n",
       "51    Singlet  [tensor(0.0816), tensor(0.0795), tensor(0.0970...           1   \n",
       "6622  Singlet  [tensor(0.1591), tensor(0.1353), tensor(0.1500...           1   \n",
       "9400  Singlet  [tensor(0.1166), tensor(0.1164), tensor(0.1190...           0   \n",
       "3022  Singlet  [tensor(0.1287), tensor(0.1243), tensor(0.1208...           0   \n",
       "1224  Singlet  [tensor(0.1595), tensor(0.1544), tensor(0.1538...           1   \n",
       "...       ...                                                ...         ...   \n",
       "153   Singlet  [tensor(0.0396), tensor(0.0453), tensor(0.0609...           1   \n",
       "9812  Doublet  [tensor(0.1147), tensor(0.1244), tensor(0.1273...           0   \n",
       "7403  Doublet  [tensor(0.1007), tensor(0.1204), tensor(0.1291...           1   \n",
       "7724  Singlet  [tensor(0.1171), tensor(0.0756), tensor(0.1007...           0   \n",
       "8570  Singlet  [tensor(0.0233), tensor(0.0371), tensor(0.0493...           0   \n",
       "\n",
       "      reconstruction_error  \n",
       "51                0.190615  \n",
       "6622              0.272947  \n",
       "9400              0.315536  \n",
       "3022              0.330162  \n",
       "1224              0.248079  \n",
       "...                    ...  \n",
       "153               0.297780  \n",
       "9812              0.316925  \n",
       "7403              0.307266  \n",
       "7724              0.365307  \n",
       "8570              0.320792  \n",
       "\n",
       "[2914 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extract_error_threshold(val_dataloader, method=\"percentile\", value=85)\n",
    "output = model.predict(test)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot out the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tn': np.int64(1234),\n",
       " 'fp': np.int64(223),\n",
       " 'fn': np.int64(334),\n",
       " 'tp': np.int64(1123),\n",
       " 'precision': np.float64(0.8343239227340268),\n",
       " 'recall': np.float64(0.7707618393960192),\n",
       " 'f1': np.float64(0.8012843382090618)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "truth = [ 0 if value ==1 else 1 for value in  output['classification']]\n",
    "predictions = [0 if val ==1 else 1 for val in  output['prediction']]\n",
    "\n",
    "\n",
    "\n",
    "perf_metrics = evaluate_perf_utils.evaluate_performance(truth, predictions)\n",
    "tn = perf_metrics['tn']\n",
    "fp = perf_metrics['fp']\n",
    "fn = perf_metrics['fn']\n",
    "tp = perf_metrics['tp']\n",
    "precision = perf_metrics['precision']\n",
    "recall = perf_metrics['recall']\n",
    "f1 = perf_metrics['f1']\n",
    "\n",
    "\n",
    "perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8088538091969801\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tn': np.int64(0), 'fp': np.int64(0), 'fn': np.int64(128), 'tp': np.int64(304), 'precision': np.float64(1.0), 'recall': np.float64(0.7037037037037037), 'f1': np.float64(0.8260869565217391)}\n",
      "tp: 0.7037037037037037\n",
      "fn: 0.2962962962962963\n"
     ]
    }
   ],
   "source": [
    "predictions_partial_scans = output[output[\"classification\"] > 1][\"prediction\"].tolist()\n",
    "predictions_partial_scans = [0 if val == 1 else 1 for val in predictions_partial_scans]\n",
    "\n",
    "partial_truths = np.ones(len(predictions_partial_scans))\n",
    "\n",
    "parital_perf_metrics = evaluate_perf_utils.evaluate_performance(\n",
    "    partial_truths, predictions_partial_scans\n",
    ")\n",
    "parital_perf_metrics\n",
    "\n",
    "# print out percentages\n",
    "print(parital_perf_metrics)\n",
    "print(f\"tp: {parital_perf_metrics['tp']/len(partial_truths)}\")\n",
    "print(f\"fn: {parital_perf_metrics['fn']/len(partial_truths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tn': np.int64(0), 'fp': np.int64(0), 'fn': np.int64(206), 'tp': np.int64(819), 'precision': np.float64(1.0), 'recall': np.float64(0.7990243902439025), 'f1': np.float64(0.8882863340563991)}\n",
      "tp: 0.7990243902439025\n",
      "fn: 0.20097560975609757\n"
     ]
    }
   ],
   "source": [
    "predictions_partial_scans = output[output[\"classification\"] < 1][\"prediction\"].tolist()\n",
    "predictions_partial_scans = [0 if val == 1 else 1 for val in predictions_partial_scans]\n",
    "\n",
    "partial_truths = np.ones(len(predictions_partial_scans))\n",
    "\n",
    "parital_perf_metrics = evaluate_perf_utils.evaluate_performance(\n",
    "    partial_truths, predictions_partial_scans\n",
    ")\n",
    "parital_perf_metrics\n",
    "\n",
    "# print out percentages\n",
    "print(parital_perf_metrics)\n",
    "print(f\"tp: {parital_perf_metrics['tp']/len(partial_truths)}\")\n",
    "print(f\"fn: {parital_perf_metrics['fn']/len(partial_truths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot out the confusion matrix and the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "tickvals": [
           0,
           100
          ],
          "title": {
           "text": "Percentage"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(253, 224, 197)"
          ],
          [
           0.16666666666666666,
           "rgb(250, 203, 166)"
          ],
          [
           0.3333333333333333,
           "rgb(248, 181, 139)"
          ],
          [
           0.5,
           "rgb(245, 158, 114)"
          ],
          [
           0.6666666666666666,
           "rgb(242, 133, 93)"
          ],
          [
           0.8333333333333334,
           "rgb(239, 106, 76)"
          ],
          [
           1,
           "rgb(235, 74, 64)"
          ]
         ],
         "showscale": true,
         "type": "heatmap",
         "x": [
          "Pred: 0",
          "Pred: 1"
         ],
         "y": [
          "True: 0",
          "True: 1"
         ],
         "z": [
          [
           42.34728894989705,
           7.652711050102952
          ],
          [
           11.461908030199039,
           38.53809196980096
          ]
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "center",
          "font": {
           "color": "black",
           "size": 14
          },
          "showarrow": false,
          "text": "42.35%",
          "x": 0,
          "y": 0
         },
         {
          "align": "center",
          "font": {
           "color": "black",
           "size": 14
          },
          "showarrow": false,
          "text": "7.65%",
          "x": 1,
          "y": 0
         },
         {
          "align": "center",
          "font": {
           "color": "black",
           "size": 14
          },
          "showarrow": false,
          "text": "11.46%",
          "x": 0,
          "y": 1
         },
         {
          "align": "center",
          "font": {
           "color": "black",
           "size": 14
          },
          "showarrow": false,
          "text": "38.54%",
          "x": 1,
          "y": 1
         }
        ],
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Confusion Matrix (Percentage)"
        },
        "width": 500,
        "xaxis": {
         "title": {
          "text": "Predicted Labels"
         }
        },
        "yaxis": {
         "title": {
          "text": "True Labels"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "cm = np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "total = cm.sum()\n",
    "cm_percentage = cm / total * 100\n",
    "\n",
    "# Create a heatmap for the confusion matrix with percentages\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm_percentage,\n",
    "        x=[\"Pred: 0\", \"Pred: 1\"],\n",
    "        y=[\"True: 0\", \"True: 1\"],\n",
    "        colorscale=\"Peach\",  # A more pleasant color palette\n",
    "        colorbar=dict(title=\"Percentage\", tickvals=[0, 100]),\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add titles, labels, and precision, recall, F1 score as annotation\n",
    "fig.update_layout(\n",
    "    title=\"Confusion Matrix (Percentage)\",\n",
    "    xaxis_title=\"Predicted Labels\",\n",
    "    yaxis_title=\"True Labels\",\n",
    "    width=500,  # Set the width of the plot\n",
    "    height=400,  # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Add text annotations to show percentage values in each cell\n",
    "for i in range(cm_percentage.shape[0]):\n",
    "    for j in range(cm_percentage.shape[1]):\n",
    "        fig.add_annotation(\n",
    "            x=j,\n",
    "            y=i,\n",
    "            text=f\"{cm_percentage[i, j]:.2f}%\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=14, color=\"black\"),\n",
    "            align=\"center\",\n",
    "        )\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9997\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m px\u001b[38;5;241m.\u001b[39mimshow(test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m9997\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m80\u001b[39m))\n\u001b[1;32m      6\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     (model(test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m9997\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_reconstructed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/machine_learning/project/majority-vote-outlier-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/project/majority-vote-outlier-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/machine_learning/project/majority-vote-outlier-detection/models/VAE/VAE.py:124\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, compute_loss)\u001b[0m\n\u001b[1;32m    121\u001b[0m ssim_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m    123\u001b[0m     ssim_value \u001b[38;5;241m=\u001b[39m ssim(\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mx_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,  \u001b[38;5;66;03m# Assuming single-channel grayscale images\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         x_reconstructed_np[i, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    126\u001b[0m         data_range\u001b[38;5;241m=\u001b[39mx_np[i, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m x_np[i, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(),\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     ssim_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ssim_value)\n\u001b[1;32m    130\u001b[0m ssim_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model(test[\"image\"][9997])\n",
    "px.imshow(test[\"image\"][9997].view(60, 80))\n",
    "\n",
    "reconstructed = (\n",
    "    (model(test[\"image\"][9997])[\"x_reconstructed\"]).detach().numpy().reshape(60, 80)\n",
    ")\n",
    "px.imshow(reconstructed)\n",
    "model.eval()\n",
    "inlier = test[\"image\"][9400]\n",
    "outlier = test[\"image\"][9997]\n",
    "inlier_forwards = model(inlier)\n",
    "outlier_forwards = model(outlier)\n",
    "inlier_reconstructed = inlier_forwards[\"x_reconstructed\"]\n",
    "outlier_reconstructed = outlier_forwards[\"x_reconstructed\"]\n",
    "\n",
    "print(\n",
    "    f\"reconstruction error for inlier: {F.binary_cross_entropy(inlier_reconstructed, inlier,reduction='none').sum(dim=(-1))}\"\n",
    ")\n",
    "print(\n",
    "    f\"reconstruction error for outlier: {F.binary_cross_entropy(outlier_reconstructed, outlier,reduction='none').sum(dim=(-1))}\"\n",
    ")\n",
    "# show both images\n",
    "fig = px.imshow(inlier.view(60, 80))\n",
    "fig.show()\n",
    "fig = px.imshow(inlier_reconstructed.detach().numpy().reshape(60, 80))\n",
    "fig.show()\n",
    "fig = px.imshow(outlier.view(60, 80))\n",
    "fig.show()\n",
    "fig = px.imshow(outlier_reconstructed.detach().numpy().reshape(60, 80))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
